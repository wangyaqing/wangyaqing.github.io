---
permalink: /
title: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
# About Me

Dr. Yaqing Wang is currently an Associate Professor at the Beijing Institute of Mathematical Sciences and Applications (BIMSA). 
She received her Ph.D. in Computer Science and Engineering from the Hong Kong University of Science and Technology in 2019, under the supervision of Professor Lionel M. Ni and Prof. James T. Kwok. 
From 2019 to 2024, she worked as a Staff Researcher at Baidu Research.
Dr. Wang has published around 30 papers in top-tier international conferences and journals, including NeurIPS, ICML, ICLR, KDD, TheWebConf, SIGIR, IJCAI, EMNLP, TPAMI, JMLR, and TIP, with more than 4700 citations. 
Dr. Wang was named to the World's Top 2% Scientists List in 2024 and selected for the Beijing Nova Program in 2025.


# Research Interests

Dr. Yaqing Wang's research focuses on machine learning, artificial intelligence, and data science.  
Her work is fundamentally guided by the principle of parsimony, aiming to uncover refined scientific explanations that address real-world challenges in an efficient and cost-effective manner.  

Her current research interests include:
- Few-shot learning, meta-learning, and in-context learning
- Large language models and agents
- AI for Science and Mathematics (AI + X)
- Cold-start recommendation and user modeling


# Recruiting

We are always looking for talented and highly motivated **students and postdoctoral researchers** to join our team:

- **Postdoctoral Positions** through the Tsinghuaâ€“BIMSA Joint Program  
- **Ph.D. Positions** through the RUCâ€“BIMSA Joint Program

Students from **Qiuzhen College, Tsinghua University** are especially encouraged to get in touch.  
Please check [here](/recruitment/) for more details.


# ðŸŽ‰ News

**2025.06**: I am selected for the Beijing Nova Program.

**2025.05**: Our work *"PERSCEN: Learning Personalized Interaction Pattern and Scenario Preference for Multi-Scenario Matching"* has been accepted to **KDD 2025**.

**2025.01**: Our work *"Why In-Context Learning Models are Good Few-Shot Learners?"* has been accepted to **ICLR 2025**.

